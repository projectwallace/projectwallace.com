---
title: "What's new in Project Wallace: January 2023"
excerpt: It's been a pretty busy month with lots of fixes and new features.
---

<script>
	import specificity_calculator from '$lib/img/blog/2023-01-31-whats-new-jan-2023/specificity-calculator.png'
	import css_units_game from '$lib/img/blog/2023-01-31-whats-new-jan-2023/css-units-game.png'
	import code_quality_threshold from '$lib/img/blog/2023-01-31-whats-new-jan-2023/code-quality-threshold.png'
</script>

It's been a pretty busy month with lots of fixes and new features.

### Specificity Analyzer page

Checking the specificity of a CSS selector is just one of those tasks that any CSS engineer does a lot. I've added a quick [specificity calculator](/specificity-calculator) page, based on [Bramus' excellent specificity package](https://github.com/bramus/specificity).

<img src={specificity_calculator} alt="Specificity calculator page showing the specificity for a selector" width="714" height="461" loading="lazy" decoding="async">

The page is nowhere near as good or complete as [the one from Polypane](https://polypane.app/css-specificity-calculator/), but Wallace aims to be a place where you can audit all of your CSS, so it just makes sense to have our own specificity analyzer here.

### CSS Units game

A recent [Tweet](https://twitter.com/argyleink/status/1611394407928070146) from Adam Argyle got me thinking that it'd be fun to create a CSS-based game. So I created it! It's pretty much a copy of [HTML memory test](https://codepen.io/plfstr/full/zYqQeRw) that seemed pretty popular a while ago. The game is to remember (or guess!) all possible CSS units. There are 64 of them and I tapped out after 10 units...

<img src={css_units_game} alt="CSS Units game in progress, showing  a progress bar with 10 units guessed of 64 total" width="714" height="468" loading="lazy" decoding="async">

### Showing actuals in Code Quality analysis

A common complaint about the CSS Code Quality report was that it could use some more details. While the current analyzer doesn't provide those (yet, because I developed it in a hurry), I added more information to most panels related to selector complexity. Now you can see the actual average line plotted in your complexity chart, for example. That should clear things up a bit.

<img src={code_quality_threshold} alt="a part of the CSS Code Quality page showing a scatter plot with selector complexity, with a clear red line drawn at the average selector complexity" width="714" height="522" loading="lazy" decoding="async">

## css-analyzer uses @bramus/specificity

After a long time of doing our own specificity analysis, I finally switched over to using [a package](https://github.com/bramus/specificity) to do that. Why? Well, analyzing specificity isn't exactly difficult, but there's a lot of gotchas and edge cases. More and more I found myself copying test cases and bits of code from other places. After I spoke to Bramus last summer at CSS Day 2022 I dabbled at bit with [implementing his package](https://github.com/projectwallace/css-analyzer/pull/270/files). At first I was afraid of performance implications, but actually there's hardly any difference between his and my own implementation. They're both built on CSSTree anyway and the code was 90% similar. And now, I can benefit from his (and [my](https://github.com/bramus/specificity/commit/973abdc2906752163af4416380decf05a1fc3459) [own](https://github.com/bramus/specificity/commit/fd29effade145c3e4f3273fc5577a5b10aa229b4)) bug fixes to make sure Wallace's specificity calculations are correct!
